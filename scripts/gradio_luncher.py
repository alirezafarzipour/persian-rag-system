#!/usr/bin/env python3
"""
Gradio Interface for Persian Drug Information RAG System
Uses the best performing model: distiluse-base-multilingual-cased-v2_finetuned
"""

import gradio as gr
import os
import sys
import time
from typing import List, Tuple, Optional
import json
import warnings

warnings.filterwarnings("ignore")

# Add project path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.retrieval import RetrievalSystem
from src.llama_client import LlamaClient
from src.evaluation import RAGEvaluator

class DrugRAGSystem:
    def __init__(self):
        self.retriever = None
        self.llama_client = None
        self.is_initialized = False
        self.model_name = "paraphrase-multilingual-MiniLM-L12-v2_finetuned"
        self.chunk_type = "sentence"  # Based on best performance
        self.initialization_status = ""
        
    def initialize_system(self) -> Tuple[bool, str]:
        """Initialize the RAG system with the best performing model"""
        try:
            status_messages = []
            
            # Step 1: Check prerequisites
            status_messages.append("ğŸ” Ø¨Ø±Ø±Ø³ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²...")
            
            chunk_file = "data/processed/drugs_sentence_chunks.csv"
            faiss_index = f"results/faiss/{self.model_name}_drugs_{self.chunk_type}_chunks.index"
            model_path = f"models/{self.model_name}"
            
            if not os.path.exists(chunk_file):
                return False, f"âŒ ÙØ§ÛŒÙ„ chunk ÛŒØ§ÙØª Ù†Ø´Ø¯: {chunk_file}"
            
            if not os.path.exists(faiss_index):
                # Try generic index as fallback
                generic_index = f"results/faiss/drugs_{self.chunk_type}_chunks.index"
                if os.path.exists(generic_index):
                    faiss_index = generic_index
                    status_messages.append(f"âš ï¸ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² index Ø¹Ù…ÙˆÙ…ÛŒ: {generic_index}")
                else:
                    return False, f"âŒ ÙØ§ÛŒÙ„ FAISS index ÛŒØ§ÙØª Ù†Ø´Ø¯: {faiss_index}"
            
            if not os.path.exists(model_path):
                # Try base model
                base_model = "sentence-transformers/distiluse-base-multilingual-cased-v2"
                model_path = base_model
                status_messages.append(f"âš ï¸ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ Ù¾Ø§ÛŒÙ‡: {base_model}")
            
            status_messages.append("âœ… ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³Øª")
            
            # Step 2: Initialize LLaMA client
            status_messages.append("ğŸ¤– Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø³Ø±ÙˆØ± LLaMA...")
            
            self.llama_client = LlamaClient("http://127.0.0.1:8080")
            
            # Test LLaMA connection
            test_response = self.llama_client.generate("Ø³Ù„Ø§Ù…", max_tokens=5)
            if not test_response:
                return False, "âŒ Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø³Ø±ÙˆØ± LLaMA Ø¨Ø±Ù‚Ø±Ø§Ø± Ù†Ø´Ø¯. Ù„Ø·ÙØ§Ù‹ Ø³Ø±ÙˆØ± Ø±Ø§ Ø±ÙˆØ´Ù† Ú©Ù†ÛŒØ¯."
            
            status_messages.append("âœ… Ø§ØªØµØ§Ù„ Ø¨Ù‡ LLaMA Ø¨Ø±Ù‚Ø±Ø§Ø± Ø´Ø¯")
            
            # Step 3: Initialize retrieval system
            status_messages.append("ğŸ” Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ...")
            
            self.retriever = RetrievalSystem(
                method="tfidf",
                model_path=model_path,
                device="cuda" if os.path.exists("/usr/local/cuda") else "cpu"
            )
            
            if not self.retriever.load_chunks_and_index(chunk_file, faiss_index):
                return False, "âŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ùˆ index Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯"
            
            status_messages.append("âœ… Ø³ÛŒØ³ØªÙ… Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯")
            
            # Step 4: Final test
            status_messages.append("ğŸ§ª ØªØ³Øª Ù†Ù‡Ø§ÛŒÛŒ Ø³ÛŒØ³ØªÙ…...")
            
            test_contexts, _ = self.retriever.get_contexts_for_rag("Ø¢Ø³Ù¾Ø±ÛŒÙ† Ú†ÛŒØ³ØªØŸ", top_k=3)
            if not test_contexts:
                return False, "âŒ ØªØ³Øª Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯"
            
            test_answer = self.llama_client.answer_question("Ø¢Ø³Ù¾Ø±ÛŒÙ† Ú†ÛŒØ³ØªØŸ", test_contexts)
            if not test_answer:
                return False, "âŒ ØªØ³Øª ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯"
            
            status_messages.append("âœ… ØªØ³Øª Ù†Ù‡Ø§ÛŒÛŒ Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯")
            status_messages.append("ğŸ‰ Ø³ÛŒØ³ØªÙ… Ø¢Ù…Ø§Ø¯Ù‡ Ù¾Ø§Ø³Ø®â€ŒÚ¯ÙˆÛŒÛŒ Ø§Ø³Øª!")
            
            self.is_initialized = True
            self.initialization_status = "\n".join(status_messages)
            
            return True, self.initialization_status
            
        except Exception as e:
            error_msg = f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ: {str(e)}"
            return False, error_msg
    
    def ask_question(self, question: str, num_contexts: int = 5) -> Tuple[str, str, str, List[str]]:
        """Process user question and return answer with details"""
        
        if not self.is_initialized:
            return (
                "âš ï¸ Ø³ÛŒØ³ØªÙ… Ù‡Ù†ÙˆØ² Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª. Ù„Ø·ÙØ§Ù‹ Ø§Ø¨ØªØ¯Ø§ Ø³ÛŒØ³ØªÙ… Ø±Ø§ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯.",
                "",
                "Ø³ÛŒØ³ØªÙ… ØºÛŒØ±ÙØ¹Ø§Ù„",
                []
            )
        
        if not question or not question.strip():
            return "Ù„Ø·ÙØ§Ù‹ Ø³ÙˆØ§Ù„ Ø®ÙˆØ¯ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯.", "", "Ø³ÙˆØ§Ù„ Ø®Ø§Ù„ÛŒ", []
        
        question = question.strip()
        
        try:
            # Step 1: Retrieve relevant contexts
            start_time = time.time()
            contexts, metadata = self.retriever.get_contexts_for_rag(
                question, 
                top_k=num_contexts,
                max_context_length=3000
            )
            retrieval_time = time.time() - start_time
            
            if not contexts:
                return (
                    "Ù…ØªØ£Ø³ÙØ§Ù†Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø±ØªØ¨Ø·ÛŒ Ø¯Ø± Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ ÛŒØ§ÙØª Ù†Ø´Ø¯.",
                    "Ù‡ÛŒÚ† Ù…ØªÙ† Ù…Ø±ØªØ¨Ø·ÛŒ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ù†Ø´Ø¯",
                    "Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ù†Ø§Ù…ÙˆÙÙ‚",
                    []
                )
            
            # Step 2: Generate answer
            start_time = time.time()
            answer = self.llama_client.answer_question(question, contexts)
            generation_time = time.time() - start_time
            
            if not answer:
                answer = "Ù…ØªØ£Ø³ÙØ§Ù†Ù‡ Ù‚Ø§Ø¯Ø± Ø¨Ù‡ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ù†ÛŒØ³ØªÙ…. Ù„Ø·ÙØ§Ù‹ Ø³ÙˆØ§Ù„ Ø±Ø§ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ú©Ù†ÛŒØ¯."
            
            # Step 3: Prepare context information
            context_info = []
            for i, (context, meta) in enumerate(zip(contexts, metadata)):
                context_preview = context[:200] + "..." if len(context) > 200 else context
                context_info.append(f"ğŸ“„ Ù…ØªÙ† {i+1} (Ø§Ù…ØªÛŒØ§Ø²: {meta['score']:.4f}):\n{context_preview}")
            
            # Step 4: Prepare processing details
            processing_details = f"""
â±ï¸ **Ø²Ù…Ø§Ù† Ù¾Ø±Ø¯Ø§Ø²Ø´:**
- Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ù…ØªÙˆÙ†: {retrieval_time:.3f} Ø«Ø§Ù†ÛŒÙ‡
- ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®: {generation_time:.3f} Ø«Ø§Ù†ÛŒÙ‡  
- Ú©Ù„: {retrieval_time + generation_time:.3f} Ø«Ø§Ù†ÛŒÙ‡

ğŸ” **Ø¬Ø²Ø¦ÛŒØ§Øª Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ:**
- ØªØ¹Ø¯Ø§Ø¯ Ù…ØªÙˆÙ† Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø´Ø¯Ù‡: {len(contexts)}
- Ù…Ø¯Ù„ embedding: {self.model_name}
- Ù†ÙˆØ¹ chunking: {self.chunk_type}

ğŸ“Š **Ú©ÛŒÙÛŒØª Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ:**
- Ø¨Ø§Ù„Ø§ØªØ±ÛŒÙ† Ø§Ù…ØªÛŒØ§Ø²: {max(m['score'] for m in metadata):.4f}
- Ù¾Ø§ÛŒÛŒÙ†â€ŒØªØ±ÛŒÙ† Ø§Ù…ØªÛŒØ§Ø²: {min(m['score'] for m in metadata):.4f}
- Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø§Ù…ØªÛŒØ§Ø²: {sum(m['score'] for m in metadata) / len(metadata):.4f}
"""
            
            processing_status = f"âœ… Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÙˆÙÙ‚ Ø¯Ø± {retrieval_time + generation_time:.3f} Ø«Ø§Ù†ÛŒÙ‡"
            
            return answer, processing_details, processing_status, context_info
            
        except Exception as e:
            error_msg = f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø³ÙˆØ§Ù„: {str(e)}"
            return error_msg, "", "Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´", []

# Initialize RAG system
rag_system = DrugRAGSystem()

def initialize_system_wrapper():
    """Wrapper for system initialization"""
    success, message = rag_system.initialize_system()
    if success:
        return (
            "âœ… Ø³ÛŒØ³ØªÙ… Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯!",
            message,
            gr.update(interactive=True),  # Enable question input
            gr.update(interactive=True),  # Enable ask button
        )
    else:
        return (
            "âŒ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯",
            message,
            gr.update(interactive=False),  # Keep question input disabled
            gr.update(interactive=False),  # Keep ask button disabled
        )

def ask_question_wrapper(question: str, num_contexts: int):
    """Wrapper for asking questions"""
    answer, details, status, contexts = rag_system.ask_question(question, num_contexts)
    
    # Format contexts for display
    contexts_display = "\n\n".join(contexts) if contexts else "Ù‡ÛŒÚ† Ù…ØªÙ† Ù…Ø±ØªØ¨Ø·ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯"
    
    return answer, details, status, contexts_display

# Create Gradio interface
with gr.Blocks(
    title="Ø³ÛŒØ³ØªÙ… Ù¾Ø§Ø³Ø®â€ŒÚ¯ÙˆÛŒÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¯Ø§Ø±ÙˆÙ‡Ø§", 
    theme=gr.themes.Soft(),
    css="""
    .rtl { direction: rtl; text-align: right; }
    .status-success { color: #28a745; font-weight: bold; }
    .status-error { color: #dc3545; font-weight: bold; }
    .answer-box { font-size: 16px; line-height: 1.6; }
    """
) as demo:
    
    gr.Markdown("""
    # ğŸ¥ Ø³ÛŒØ³ØªÙ… Ù¾Ø§Ø³Ø®â€ŒÚ¯ÙˆÛŒÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¯Ø§Ø±ÙˆÙ‡Ø§
    
    Ø§ÛŒÙ† Ø³ÛŒØ³ØªÙ… Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªÚ©Ù†ÙˆÙ„ÙˆÚ˜ÛŒ RAG (Retrieval-Augmented Generation) Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ø¯Ù‚ÛŒÙ‚ Ùˆ Ø¹Ù„Ù…ÛŒ Ø¯Ø± Ù…ÙˆØ±Ø¯ Ø¯Ø§Ø±ÙˆÙ‡Ø§ Ø§Ø±Ø§Ø¦Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.
    
    **Ù…Ø´Ø®ØµØ§Øª ÙÙ†ÛŒ:**
    - Ù…Ø¯Ù„ embedding: paraphrase-multilingual-MiniLM-L12-v2_finetuned
    - Ù…Ø¯Ù„ Ø²Ø¨Ø§Ù†: LLaMA 3.2 1B finetuned (local with llama.cpp)
    - Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡: Drugs.pdf
    """, elem_classes=["rtl"])
    
    with gr.Row():
        with gr.Column(scale=2):
            # Initialization section
            gr.Markdown("## ğŸš€ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ…", elem_classes=["rtl"])
            
            with gr.Row():
                init_button = gr.Button("ğŸ”§ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ…", variant="primary", size="lg")
            
            init_status = gr.Textbox(
                label="ÙˆØ¶Ø¹ÛŒØª Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ",
                value="Ø³ÛŒØ³ØªÙ… Ù‡Ù†ÙˆØ² Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª",
                elem_classes=["rtl"],
                interactive=False
            )
            
            init_details = gr.Textbox(
                label="Ø¬Ø²Ø¦ÛŒØ§Øª Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ", 
                lines=8,
                elem_classes=["rtl"],
                interactive=False
            )
            
            # Question section
            gr.Markdown("## ğŸ’¬ Ø³ÙˆØ§Ù„ Ùˆ Ø¬ÙˆØ§Ø¨", elem_classes=["rtl"])
            
            question_input = gr.Textbox(
                label="Ø³ÙˆØ§Ù„ Ø´Ù…Ø§",
                placeholder="Ù…Ø«Ø§Ù„: Ø¢Ø³Ù¾Ø±ÛŒÙ† Ú†Ù‡ Ø¹ÙˆØ§Ø±Ø¶ÛŒ Ø¯Ø§Ø±Ø¯ØŸ",
                lines=2,
                elem_classes=["rtl"],
                interactive=False  # Disabled until initialization
            )
            
            with gr.Row():
                num_contexts = gr.Slider(
                    minimum=3,
                    maximum=10,
                    value=5,
                    step=1,
                    label="ØªØ¹Ø¯Ø§Ø¯ Ù…ØªÙˆÙ† Ù…Ø±Ø¬Ø¹",
                    elem_classes=["rtl"]
                )
                
                ask_button = gr.Button(
                    "ğŸ” Ù¾Ø±Ø³Ø´",
                    variant="secondary",
                    interactive=False  # Disabled until initialization
                )
        
        with gr.Column(scale=3):
            # Answer section
            gr.Markdown("## ğŸ¤– Ù¾Ø§Ø³Ø® Ø³ÛŒØ³ØªÙ…", elem_classes=["rtl"])
            
            answer_output = gr.Textbox(
                label="Ù¾Ø§Ø³Ø®",
                lines=8,
                elem_classes=["rtl", "answer-box"],
                interactive=False,
                rtl=True  # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§ÛŒÙ† Ø®Ø·
            )

            processing_status = gr.Textbox(
                label="ÙˆØ¶Ø¹ÛŒØª Ù¾Ø±Ø¯Ø§Ø²Ø´",
                elem_classes=["rtl"],
                interactive=False
            )
            
            # Processing details (collapsible)
            with gr.Accordion("ğŸ“Š Ø¬Ø²Ø¦ÛŒØ§Øª Ù¾Ø±Ø¯Ø§Ø²Ø´", open=False):
                processing_details = gr.Textbox(
                    label="Ø¬Ø²Ø¦ÛŒØ§Øª ÙÙ†ÛŒ",
                    lines=10,
                    elem_classes=["rtl"],
                    interactive=False
                )
            
            # Retrieved contexts (collapsible)
            with gr.Accordion("ğŸ“„ Ù…ØªÙˆÙ† Ù…Ø±Ø¬Ø¹", open=False):
                contexts_output = gr.Textbox(
                    label="Ù…ØªÙˆÙ† Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø´Ø¯Ù‡",
                    lines=15,
                    elem_classes=["rtl"],
                    interactive=False,
                    rtl=True # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§ÛŒÙ† Ø®Ø·
                )
    
    # Event handlers
    init_button.click(
        initialize_system_wrapper,
        outputs=[init_status, init_details, question_input, ask_button]
    )
    
    ask_button.click(
        ask_question_wrapper,
        inputs=[question_input, num_contexts],
        outputs=[answer_output, processing_details, processing_status, contexts_output]
    )
    
    # Enable enter key for asking questions
    question_input.submit(
        ask_question_wrapper,
        inputs=[question_input, num_contexts], 
        outputs=[answer_output, processing_details, processing_status, contexts_output]
    )
    

# Launch configuration
if __name__ == "__main__":
    print("ğŸš€ Starting Drug RAG System Interface...")
    print("ğŸŒ Interface will be available at: http://localhost:7860")
    print("ğŸ”§ Make sure LLaMA server is running on localhost:8080")
    print("ğŸ“ Make sure all required files are in place")
    
    demo.launch(
        server_name="127.0.0.1",
        server_port=7860,
        share=False,  # Set to True if you want public sharing
        show_error=True,
        # show_tips=True,
        # enable_queue=True,
        max_threads=10
    )