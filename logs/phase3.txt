W0829 15:36:39.597000 9976 torch\distributed\elastic\multiprocessing\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.
=== PHASE 3: PDF Chunking and Vector DB (Memory Optimized) ===

âœ“ GPU Available: NVIDIA GeForce RTX 3060 (12.0GB)
Step 1: Extracting text from PDF...
Extracting text from data/raw/Drugs.pdf
Extracted 69481 characters from PDF
âœ“ Extracted 69481 characters from PDF

Step 2: Chunking PDF text...
Processing PDF document with memory-optimized chunking...
PDF text length: 69481 characters

--- Word-based chunking ---
Performing memory-optimized word-based chunking...
  Processed segment 1, chunks so far: 84
  Processed segment 1, chunks so far: 126
âœ“ Created 126 word-based chunks

--- Sentence-based chunking ---
Performing memory-optimized sentence-based chunking...
  Processed segment 1
âœ“ Created 121 sentence-based chunks

âœ“ PDF processing completed:
  - Total characters: 69481
  - Estimated total words: 12752
  - Word-based chunks: 126
  - Sentence-based chunks: 121

Chunking Results:
  Word chunks: 126
  Sentence chunks: 121
âœ“ Chunks saved to data/processed/drugs_word_chunks.csv
âœ“ Chunks saved to data/processed/drugs_sentence_chunks.csv

ðŸ“Š CHUNKING STATISTICS:
Word-based chunks: {'total_chunks': 126, 'avg_words_per_chunk': 148.4047619047619, 'min_words_per_chunk': 26, 'max_words_per_chunk': 150, 'total_words': 18699, 'chunk_type': 'word_based'}
Sentence-based chunks: {'total_chunks': 121, 'avg_words_per_chunk': 105.38842975206612, 'min_words_per_chunk': 36, 'max_words_per_chunk': 341, 'total_words': 12752, 'chunk_type': 'sentence_based'}       

Step 3: Loading best embedding model...
âœ“ Using trained model: distiluse-base-multilingual-cased-v2_finetuned
âœ“ Model loaded on cuda

Step 4: Generating embeddings with memory optimization...
  Generating word-based embeddings...
Generating embeddings for 126 texts...
âœ“ Generated embeddings shape: (126, 512)
  Generating sentence-based embeddings...
Generating embeddings for 121 texts...
âœ“ Generated embeddings shape: (121, 512)
âœ“ Generated word embeddings: (126, 512)
âœ“ Generated sentence embeddings: (121, 512)

Step 5: Setting up FAISS indexes...
  Creating FAISS index for word-based chunks...
Setting up FAISS index for 126 embeddings...
  Adding embeddings to index...
âœ“ FAISS index created successfully
  Creating FAISS index for sentence-based chunks...
Setting up FAISS index for 121 embeddings...
  Adding embeddings to index...
âœ“ FAISS index created successfully
âœ“ Word FAISS index saved
âœ“ Sentence FAISS index saved
âœ“ FAISS setup completed in 0.00s

Step 6: Setting up Chroma collections...
  Creating Chroma collection for word-based chunks...
Setting up Chroma collection: drugs_word_chunks
  Adding 126 documents in 1 batches...
âœ“ Chroma collection 'drugs_word_chunks' created with 126 items
  Creating Chroma collection for sentence-based chunks...
Setting up Chroma collection: drugs_sentence_chunks
  Adding 121 documents in 1 batches...
âœ“ Chroma collection 'drugs_sentence_chunks' created with 121 items
âœ“ Chroma setup completed in 1.15s

Step 7: Testing index quality...
âœ“ Word FAISS search test completed
  Top 3 distances: [0.08307499 0.18699469 0.28348845]
âœ“ Sentence FAISS search test completed
  Top 3 distances: [0.4591689 0.4774152 0.4929902]
C:\Users\alire\.cache\chroma\onnx_models\all-MiniLM-L6-v2\onnx.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79.3M/79.3M [30:01<00:00, 46.2kiB/s]
Error testing word Chroma: Collection expecting embedding with dimension of 512, got 384
Error testing sentence Chroma: Collection expecting embedding with dimension of 512, got 384

Step 8: Saving results and statistics...
Results saved to results/phase3_pdf_processing_results.json

==================================================
ðŸŽ‰ PHASE 3 COMPLETED!
âœ… Device used: cuda
âœ… Word chunks: 126 chunks
âœ… Sentence chunks: 121 chunks
âœ… Total memory used: 0.5 MB
âœ… FAISS indexes: Word=True, Sentence=True
âœ… Chroma collections: Word=True, Sentence=True
âœ… All results saved to results/

ðŸ“ FILES CREATED:
  - data/processed/drugs_word_chunks.csv
  - data/processed/drugs_sentence_chunks.csv
  - results/faiss/drugs_word_chunks.index
  - results/faiss/drugs_sentence_chunks.index
  - chroma_db/ (Chroma database directory)
  - results/phase3_pdf_processing_results.json

âœ… Phase 3 executed successfully!

W0830 10:10:55.118000 34952 torch\distributed\elastic\multiprocessing\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.
=== PHASE 3: PDF Chunking and Vector DB (Memory Optimized) ===

âœ“ GPU Available: NVIDIA GeForce RTX 3060 (12.0GB)
Step 1: Extracting text from PDF...
Extracting text from data/raw/Drugs.pdf
Extracted 69842 characters from PDF
âœ“ Extracted 69842 characters from PDF

Step 2: Chunking PDF text...
Processing PDF document with memory-optimized chunking...
PDF text length: 69842 characters

--- Word-based chunking ---
Performing memory-optimized word-based chunking...
  Processed segment 1, chunks so far: 83
  Processed segment 1, chunks so far: 125
âœ“ Created 125 word-based chunks

--- Sentence-based chunking ---
Performing memory-optimized sentence-based chunking...
  Processed segment 1
âœ“ Created 121 sentence-based chunks

âœ“ PDF processing completed:
  - Total characters: 69842
  - Estimated total words: 12789
  - Word-based chunks: 125
  - Sentence-based chunks: 121

Chunking Results:
  Word chunks: 125
  Sentence chunks: 121
âœ“ Chunks saved to data/processed/drugs_word_chunks.csv
âœ“ Chunks saved to data/processed/drugs_sentence_chunks.csv

ðŸ“Š CHUNKING STATISTICS:
Word-based chunks: {'total_chunks': 125, 'avg_words_per_chunk': 149.336, 'min_words_per_chunk': 92, 'max_words_per_chunk': 150, 'total_words': 18667, 'chunk_type': 'word_based'}
Sentence-based chunks: {'total_chunks': 121, 'avg_words_per_chunk': 105.69421487603306, 'min_words_per_chunk': 29, 'max_words_per_chunk': 407, 'total_words': 12789, 'chunk_type': 'sentence_based'}       

Step 3: Loading best embedding model...
âœ“ Using trained model: distiluse-base-multilingual-cased-v2_finetuned
âœ“ Model loaded on cuda

Step 4: Generating embeddings with memory optimization...
  Generating word-based embeddings...
Generating embeddings for 125 texts...
âœ“ Generated embeddings shape: (125, 512)
  Generating sentence-based embeddings...
Generating embeddings for 121 texts...
âœ“ Generated embeddings shape: (121, 512)
âœ“ Generated word embeddings: (125, 512)
âœ“ Generated sentence embeddings: (121, 512)

Step 5: Setting up FAISS indexes...
  Creating FAISS index for word-based chunks...
Setting up FAISS index for 125 embeddings...
  Adding embeddings to index...
âœ“ FAISS index created successfully
  Creating FAISS index for sentence-based chunks...
Setting up FAISS index for 121 embeddings...
  Adding embeddings to index...
âœ“ FAISS index created successfully
âœ“ Word FAISS index saved
âœ“ Sentence FAISS index saved
âœ“ FAISS setup completed in 0.00s

Step 6: Setting up Chroma collections...
  Creating Chroma collection for word-based chunks...
Setting up Chroma collection: drugs_word_chunks
  Previous collection deleted
  Adding 125 documents in 1 batches...
âœ“ Chroma collection 'drugs_word_chunks' created with 125 items
  Creating Chroma collection for sentence-based chunks...
Setting up Chroma collection: drugs_sentence_chunks
  Previous collection deleted
  Adding 121 documents in 1 batches...
âœ“ Chroma collection 'drugs_sentence_chunks' created with 121 items
âœ“ Chroma setup completed in 1.65s

Step 7: Testing index quality...
âœ“ Word FAISS search test completed
  Top 3 distances: [0.27873075 0.37690514 0.39904034]
âœ“ Sentence FAISS search test completed
  Top 3 distances: [0.3322068  0.4418366  0.44654197]
Error testing word Chroma: Collection expecting embedding with dimension of 512, got 384
Error testing sentence Chroma: Collection expecting embedding with dimension of 512, got 384

Step 8: Saving results and statistics...
Results saved to results/phase3_pdf_processing_results.json

==================================================
ðŸŽ‰ PHASE 3 COMPLETED!
âœ… Device used: cuda
âœ… Word chunks: 125 chunks
âœ… Sentence chunks: 121 chunks
âœ… Word chunks: 125 chunks
âœ… Sentence chunks: 121 chunks
âœ… Sentence chunks: 121 chunks
âœ… Total memory used: 0.5 MB
âœ… Total memory used: 0.5 MB
âœ… FAISS indexes: Word=True, Sentence=True
âœ… FAISS indexes: Word=True, Sentence=True
âœ… Chroma collections: Word=True, Sentence=True
âœ… Chroma collections: Word=True, Sentence=True
âœ… All results saved to results/
âœ… All results saved to results/


ðŸ“ FILES CREATED:
  - data/processed/drugs_word_chunks.csv
  - data/processed/drugs_sentence_chunks.csv
  - results/faiss/drugs_word_chunks.index
  - results/faiss/drugs_sentence_chunks.index
  - chroma_db/ (Chroma database directory)
  - results/phase3_pdf_processing_results.json

âœ… Phase 3 executed successfully!